{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "ITb6gEIOBYoG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "323566c4-8910-4cbb-8094-346c39ae286f"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9c8860d51523>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdsets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo install torch, click the button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "FnrkIXhHTRb9",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "3aa16727-25ab-43e7-a069-f259d3e3ff9d"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-88e10be0-aaaa-459d-adbf-7ce7c93f96e0\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-88e10be0-aaaa-459d-adbf-7ce7c93f96e0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Test.csv to Test.csv\n",
            "Saving Train.csv to Train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "d34GeuNqTVbj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(\"Train.csv\")\n",
        "test = pd.read_csv(\"Test.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "79RD0nQ_Tv_c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "7c309950-1729-413e-96d9-1c4d76f4dabe"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "class NewTrain():\n",
        "    def __init__(self):\n",
        "      train_X = train.drop(\"Result\",axis=1).as_matrix().reshape(-1,1,90,90)\n",
        "      train_Y = train.Result.as_matrix().tolist()\n",
        "      self.datalist = train_X\n",
        "      self.labellist = train_Y\n",
        "    def __getitem__(self, index):\n",
        "        return torch.Tensor(self.datalist[index].astype(float)), self.labellist[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.datalist.shape[0]\n",
        "      \n",
        "\n",
        "    \n",
        "train_data=NewTrain()\n",
        "print(len(train_data))\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_data,batch_size=1, shuffle=True,num_workers=2)\n",
        "print(len(train_loader))     "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-P1bg7oqCA6n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "92cf2103-85b7-4dd0-bf48-368f0d426baf"
      },
      "cell_type": "code",
      "source": [
        "class NewTest():\n",
        "    def __init__(self):\n",
        "      test_X = test.drop(\"Result\",axis=1).as_matrix().reshape(-1,1,90,90)\n",
        "      test_Y = test.Result.as_matrix().tolist()\n",
        "      self.datalist = test_X\n",
        "      self.labellist = test_Y\n",
        "    def __getitem__(self, index):\n",
        "        return torch.Tensor(self.datalist[index].astype(float)), self.labellist[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.datalist.shape[0]\n",
        "\n",
        "test_data=NewTest()\n",
        "print(len(test_data))\n",
        "test_loader = torch.utils.data.DataLoader( dataset=test_data,batch_size=1, shuffle=True)\n",
        "print(len(test_loader))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "45\n",
            "45\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l1V0B2IFYxCN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Pro(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Pro, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=7)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=7)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(6480, 50)\n",
        "        self.fc2 = nn.Linear(50, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 6480)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "      \n",
        "def train(model,train_loader,optimizer,epoch):\n",
        "    model.train()\n",
        "    train_loss=0\n",
        "    for batch_idx, (inputs,target) in enumerate(train_loader):\n",
        "      inputs,target = torch.autograd.Variable(inputs), torch.autograd.Variable(target)\n",
        "      optimizer.zero_grad()\n",
        "      output=model(inputs)\n",
        "      loss =criterion(output.double(),target.long())\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      train_loss+=loss.data[0]\n",
        "      if batch_idx % 1 ==0:\n",
        "        print ('Epoch [%d/%d],  Loss: %.4f' %(epoch+1, 5, loss.data[0]))\n",
        "        train_loss=0\n",
        "        \n",
        "def test(model,test_loader):\n",
        "    model.eval()\n",
        "    test_loss=0\n",
        "    correct=0\n",
        "    for (inputs,target) in test_loader:\n",
        "        inputs,target=torch.autograd.Variable(inputs), torch.autograd.Variable(target)\n",
        "        output=model(inputs)\n",
        "        print(output)\n",
        "        loss =criterion(output.double(),target.long())\n",
        "        test_loss +=loss.data[0] # sum up batch loss\n",
        "        pred =  np.argmax(output.data.cpu().numpy(), axis=1)\n",
        "        #output.max()[1].double() # get the index of the max log-probability\n",
        "        label=target.data.cpu().numpy()\n",
        "        correct += (pred ==label).sum()\n",
        "        test_loss /= len(test_loader.dataset)\n",
        "        \n",
        "    print(\"The accuracy percentage is {}\".format((correct/50)*100))\n",
        "    print('\\n')\n",
        "    print(\"The average loss is{}\".format(test_loss))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zneHiavS42Oi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11458
        },
        "outputId": "5f9d9583-1a96-43c5-c56e-51944705df52"
      },
      "cell_type": "code",
      "source": [
        "model = Pro()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "\n",
        "for epoch in range(5):\n",
        "  train(model, train_loader, optimizer, epoch)\n",
        "test(model, test_loader)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/5],  Loss: 0.6610\n",
            "Epoch [1/5],  Loss: 0.5662\n",
            "Epoch [1/5],  Loss: 1.0183\n",
            "Epoch [1/5],  Loss: 0.7698\n",
            "Epoch [1/5],  Loss: 0.7132\n",
            "Epoch [1/5],  Loss: 0.7308\n",
            "Epoch [1/5],  Loss: 0.8499\n",
            "Epoch [1/5],  Loss: 0.6917\n",
            "Epoch [1/5],  Loss: 0.7018\n",
            "Epoch [1/5],  Loss: 0.7118\n",
            "Epoch [1/5],  Loss: 0.6568\n",
            "Epoch [1/5],  Loss: 0.6627\n",
            "Epoch [1/5],  Loss: 0.6715\n",
            "Epoch [1/5],  Loss: 0.5960\n",
            "Epoch [1/5],  Loss: 0.6223\n",
            "Epoch [1/5],  Loss: 0.9057\n",
            "Epoch [1/5],  Loss: 0.8146\n",
            "Epoch [1/5],  Loss: 0.5490\n",
            "Epoch [1/5],  Loss: 0.5545\n",
            "Epoch [1/5],  Loss: 0.7494\n",
            "Epoch [1/5],  Loss: 0.4101\n",
            "Epoch [1/5],  Loss: 0.7499\n",
            "Epoch [1/5],  Loss: 0.7288\n",
            "Epoch [1/5],  Loss: 1.3355\n",
            "Epoch [1/5],  Loss: 0.6029\n",
            "Epoch [1/5],  Loss: 0.6566\n",
            "Epoch [1/5],  Loss: 0.8160\n",
            "Epoch [1/5],  Loss: 0.6619\n",
            "Epoch [1/5],  Loss: 0.7337\n",
            "Epoch [1/5],  Loss: 0.8234\n",
            "Epoch [1/5],  Loss: 0.7077\n",
            "Epoch [1/5],  Loss: 0.6722\n",
            "Epoch [1/5],  Loss: 0.7180\n",
            "Epoch [1/5],  Loss: 0.7255\n",
            "Epoch [1/5],  Loss: 0.6519\n",
            "Epoch [1/5],  Loss: 0.6888\n",
            "Epoch [1/5],  Loss: 0.6761\n",
            "Epoch [1/5],  Loss: 0.6694\n",
            "Epoch [1/5],  Loss: 0.5607\n",
            "Epoch [1/5],  Loss: 0.6658\n",
            "Epoch [1/5],  Loss: 0.7383\n",
            "Epoch [1/5],  Loss: 1.4202\n",
            "Epoch [1/5],  Loss: 0.6604\n",
            "Epoch [1/5],  Loss: 0.6500\n",
            "Epoch [1/5],  Loss: 0.6503\n",
            "Epoch [1/5],  Loss: 0.7670\n",
            "Epoch [1/5],  Loss: 0.6103\n",
            "Epoch [1/5],  Loss: 0.7807\n",
            "Epoch [1/5],  Loss: 0.6404\n",
            "Epoch [1/5],  Loss: 0.7668\n",
            "Epoch [1/5],  Loss: 0.7773\n",
            "Epoch [1/5],  Loss: 0.7716\n",
            "Epoch [1/5],  Loss: 0.6521\n",
            "Epoch [1/5],  Loss: 0.6545\n",
            "Epoch [1/5],  Loss: 0.6414\n",
            "Epoch [1/5],  Loss: 0.6291\n",
            "Epoch [1/5],  Loss: 0.7742\n",
            "Epoch [1/5],  Loss: 0.7666\n",
            "Epoch [1/5],  Loss: 0.6460\n",
            "Epoch [1/5],  Loss: 0.7581\n",
            "Epoch [1/5],  Loss: 0.6424\n",
            "Epoch [1/5],  Loss: 0.7466\n",
            "Epoch [1/5],  Loss: 0.6422\n",
            "Epoch [1/5],  Loss: 0.7564\n",
            "Epoch [1/5],  Loss: 0.7470\n",
            "Epoch [1/5],  Loss: 0.7357\n",
            "Epoch [1/5],  Loss: 0.6666\n",
            "Epoch [1/5],  Loss: 0.7342\n",
            "Epoch [1/5],  Loss: 0.7066\n",
            "Epoch [1/5],  Loss: 0.7059\n",
            "Epoch [1/5],  Loss: 0.7022\n",
            "Epoch [1/5],  Loss: 0.6957\n",
            "Epoch [1/5],  Loss: 0.6952\n",
            "Epoch [1/5],  Loss: 0.7009\n",
            "Epoch [1/5],  Loss: 0.6920\n",
            "Epoch [1/5],  Loss: 0.6893\n",
            "Epoch [1/5],  Loss: 0.7102\n",
            "Epoch [1/5],  Loss: 0.6804\n",
            "Epoch [1/5],  Loss: 0.6697\n",
            "Epoch [1/5],  Loss: 0.6456\n",
            "Epoch [1/5],  Loss: 0.6582\n",
            "Epoch [1/5],  Loss: 0.7270\n",
            "Epoch [1/5],  Loss: 0.6792\n",
            "Epoch [1/5],  Loss: 0.7408\n",
            "Epoch [1/5],  Loss: 0.7341\n",
            "Epoch [1/5],  Loss: 0.6334\n",
            "Epoch [1/5],  Loss: 0.7153\n",
            "Epoch [1/5],  Loss: 0.6633\n",
            "Epoch [1/5],  Loss: 0.7318\n",
            "Epoch [1/5],  Loss: 0.6136\n",
            "Epoch [1/5],  Loss: 0.7320\n",
            "Epoch [1/5],  Loss: 0.6830\n",
            "Epoch [1/5],  Loss: 0.8724\n",
            "Epoch [1/5],  Loss: 0.6700\n",
            "Epoch [1/5],  Loss: 0.7650\n",
            "Epoch [1/5],  Loss: 0.7244\n",
            "Epoch [1/5],  Loss: 0.6806\n",
            "Epoch [1/5],  Loss: 0.6791\n",
            "Epoch [1/5],  Loss: 0.7135\n",
            "Epoch [1/5],  Loss: 0.6692\n",
            "Epoch [2/5],  Loss: 0.6601\n",
            "Epoch [2/5],  Loss: 0.6473\n",
            "Epoch [2/5],  Loss: 0.7629\n",
            "Epoch [2/5],  Loss: 0.6521\n",
            "Epoch [2/5],  Loss: 0.6516\n",
            "Epoch [2/5],  Loss: 0.7406\n",
            "Epoch [2/5],  Loss: 0.7480\n",
            "Epoch [2/5],  Loss: 0.6385\n",
            "Epoch [2/5],  Loss: 0.6413\n",
            "Epoch [2/5],  Loss: 0.7599\n",
            "Epoch [2/5],  Loss: 0.6257\n",
            "Epoch [2/5],  Loss: 0.7454\n",
            "Epoch [2/5],  Loss: 0.7663\n",
            "Epoch [2/5],  Loss: 0.7302\n",
            "Epoch [2/5],  Loss: 0.7182\n",
            "Epoch [2/5],  Loss: 0.7152\n",
            "Epoch [2/5],  Loss: 0.6764\n",
            "Epoch [2/5],  Loss: 0.6824\n",
            "Epoch [2/5],  Loss: 0.7104\n",
            "Epoch [2/5],  Loss: 0.7164\n",
            "Epoch [2/5],  Loss: 0.6816\n",
            "Epoch [2/5],  Loss: 0.6837\n",
            "Epoch [2/5],  Loss: 0.6718\n",
            "Epoch [2/5],  Loss: 0.6667\n",
            "Epoch [2/5],  Loss: 0.7343\n",
            "Epoch [2/5],  Loss: 0.7239\n",
            "Epoch [2/5],  Loss: 0.6587\n",
            "Epoch [2/5],  Loss: 0.6692\n",
            "Epoch [2/5],  Loss: 0.7325\n",
            "Epoch [2/5],  Loss: 0.6618\n",
            "Epoch [2/5],  Loss: 0.6380\n",
            "Epoch [2/5],  Loss: 0.6314\n",
            "Epoch [2/5],  Loss: 0.6240\n",
            "Epoch [2/5],  Loss: 0.6312\n",
            "Epoch [2/5],  Loss: 0.5840\n",
            "Epoch [2/5],  Loss: 0.8150\n",
            "Epoch [2/5],  Loss: 0.7994\n",
            "Epoch [2/5],  Loss: 0.6066\n",
            "Epoch [2/5],  Loss: 0.7873\n",
            "Epoch [2/5],  Loss: 0.5979\n",
            "Epoch [2/5],  Loss: 0.7687\n",
            "Epoch [2/5],  Loss: 0.6233\n",
            "Epoch [2/5],  Loss: 0.6132\n",
            "Epoch [2/5],  Loss: 0.8382\n",
            "Epoch [2/5],  Loss: 0.8020\n",
            "Epoch [2/5],  Loss: 0.6297\n",
            "Epoch [2/5],  Loss: 0.6233\n",
            "Epoch [2/5],  Loss: 0.6160\n",
            "Epoch [2/5],  Loss: 0.7776\n",
            "Epoch [2/5],  Loss: 0.6149\n",
            "Epoch [2/5],  Loss: 0.7721\n",
            "Epoch [2/5],  Loss: 0.7828\n",
            "Epoch [2/5],  Loss: 0.6246\n",
            "Epoch [2/5],  Loss: 0.6309\n",
            "Epoch [2/5],  Loss: 0.7773\n",
            "Epoch [2/5],  Loss: 0.7675\n",
            "Epoch [2/5],  Loss: 0.7629\n",
            "Epoch [2/5],  Loss: 0.6337\n",
            "Epoch [2/5],  Loss: 0.6312\n",
            "Epoch [2/5],  Loss: 0.6379\n",
            "Epoch [2/5],  Loss: 0.7645\n",
            "Epoch [2/5],  Loss: 0.7538\n",
            "Epoch [2/5],  Loss: 0.7418\n",
            "Epoch [2/5],  Loss: 0.6351\n",
            "Epoch [2/5],  Loss: 0.6424\n",
            "Epoch [2/5],  Loss: 0.7634\n",
            "Epoch [2/5],  Loss: 0.7104\n",
            "Epoch [2/5],  Loss: 0.6384\n",
            "Epoch [2/5],  Loss: 0.7385\n",
            "Epoch [2/5],  Loss: 0.7009\n",
            "Epoch [2/5],  Loss: 0.6932\n",
            "Epoch [2/5],  Loss: 0.6636\n",
            "Epoch [2/5],  Loss: 0.5889\n",
            "Epoch [2/5],  Loss: 0.7036\n",
            "Epoch [2/5],  Loss: 0.7351\n",
            "Epoch [2/5],  Loss: 0.5697\n",
            "Epoch [2/5],  Loss: 0.6158\n",
            "Epoch [2/5],  Loss: 0.7251\n",
            "Epoch [2/5],  Loss: 0.6791\n",
            "Epoch [2/5],  Loss: 0.6842\n",
            "Epoch [2/5],  Loss: 0.6606\n",
            "Epoch [2/5],  Loss: 1.6433\n",
            "Epoch [2/5],  Loss: 0.7232\n",
            "Epoch [2/5],  Loss: 0.6759\n",
            "Epoch [2/5],  Loss: 0.7303\n",
            "Epoch [2/5],  Loss: 0.6833\n",
            "Epoch [2/5],  Loss: 0.7179\n",
            "Epoch [2/5],  Loss: 0.6786\n",
            "Epoch [2/5],  Loss: 0.7128\n",
            "Epoch [2/5],  Loss: 0.7096\n",
            "Epoch [2/5],  Loss: 0.7011\n",
            "Epoch [2/5],  Loss: 0.7016\n",
            "Epoch [2/5],  Loss: 0.6975\n",
            "Epoch [2/5],  Loss: 0.6976\n",
            "Epoch [2/5],  Loss: 0.6976\n",
            "Epoch [2/5],  Loss: 0.6982\n",
            "Epoch [2/5],  Loss: 0.6904\n",
            "Epoch [2/5],  Loss: 0.6856\n",
            "Epoch [2/5],  Loss: 0.7072\n",
            "Epoch [2/5],  Loss: 0.7059\n",
            "Epoch [3/5],  Loss: 0.6990\n",
            "Epoch [3/5],  Loss: 0.6893\n",
            "Epoch [3/5],  Loss: 0.6994\n",
            "Epoch [3/5],  Loss: 0.6999\n",
            "Epoch [3/5],  Loss: 0.6964\n",
            "Epoch [3/5],  Loss: 0.6849\n",
            "Epoch [3/5],  Loss: 0.6809\n",
            "Epoch [3/5],  Loss: 0.6670\n",
            "Epoch [3/5],  Loss: 0.7350\n",
            "Epoch [3/5],  Loss: 0.7292\n",
            "Epoch [3/5],  Loss: 0.7211\n",
            "Epoch [3/5],  Loss: 0.6654\n",
            "Epoch [3/5],  Loss: 0.7100\n",
            "Epoch [3/5],  Loss: 0.6805\n",
            "Epoch [3/5],  Loss: 0.7114\n",
            "Epoch [3/5],  Loss: 0.7126\n",
            "Epoch [3/5],  Loss: 0.6828\n",
            "Epoch [3/5],  Loss: 0.6844\n",
            "Epoch [3/5],  Loss: 0.6752\n",
            "Epoch [3/5],  Loss: 0.7240\n",
            "Epoch [3/5],  Loss: 0.7122\n",
            "Epoch [3/5],  Loss: 0.6750\n",
            "Epoch [3/5],  Loss: 0.7213\n",
            "Epoch [3/5],  Loss: 0.6762\n",
            "Epoch [3/5],  Loss: 0.7039\n",
            "Epoch [3/5],  Loss: 0.7067\n",
            "Epoch [3/5],  Loss: 0.6921\n",
            "Epoch [3/5],  Loss: 0.6889\n",
            "Epoch [3/5],  Loss: 0.7003\n",
            "Epoch [3/5],  Loss: 0.7005\n",
            "Epoch [3/5],  Loss: 0.6942\n",
            "Epoch [3/5],  Loss: 0.6918\n",
            "Epoch [3/5],  Loss: 0.6929\n",
            "Epoch [3/5],  Loss: 0.6945\n",
            "Epoch [3/5],  Loss: 0.6981\n",
            "Epoch [3/5],  Loss: 0.6903\n",
            "Epoch [3/5],  Loss: 0.7033\n",
            "Epoch [3/5],  Loss: 0.6777\n",
            "Epoch [3/5],  Loss: 0.7012\n",
            "Epoch [3/5],  Loss: 0.6880\n",
            "Epoch [3/5],  Loss: 0.6790\n",
            "Epoch [3/5],  Loss: 0.7064\n",
            "Epoch [3/5],  Loss: 0.6725\n",
            "Epoch [3/5],  Loss: 0.6700\n",
            "Epoch [3/5],  Loss: 0.7297\n",
            "Epoch [3/5],  Loss: 0.7299\n",
            "Epoch [3/5],  Loss: 0.7188\n",
            "Epoch [3/5],  Loss: 0.7143\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [3/5],  Loss: 0.6958\n",
            "Epoch [3/5],  Loss: 0.6955\n",
            "Epoch [3/5],  Loss: 0.6939\n",
            "Epoch [3/5],  Loss: 0.6978\n",
            "Epoch [3/5],  Loss: 0.6818\n",
            "Epoch [3/5],  Loss: 0.7040\n",
            "Epoch [3/5],  Loss: 0.7145\n",
            "Epoch [3/5],  Loss: 0.6947\n",
            "Epoch [3/5],  Loss: 0.7003\n",
            "Epoch [3/5],  Loss: 0.6834\n",
            "Epoch [3/5],  Loss: 0.6810\n",
            "Epoch [3/5],  Loss: 0.7062\n",
            "Epoch [3/5],  Loss: 0.7112\n",
            "Epoch [3/5],  Loss: 0.6719\n",
            "Epoch [3/5],  Loss: 0.7063\n",
            "Epoch [3/5],  Loss: 0.6824\n",
            "Epoch [3/5],  Loss: 0.6803\n",
            "Epoch [3/5],  Loss: 0.7184\n",
            "Epoch [3/5],  Loss: 0.6658\n",
            "Epoch [3/5],  Loss: 0.7146\n",
            "Epoch [3/5],  Loss: 0.6763\n",
            "Epoch [3/5],  Loss: 0.6708\n",
            "Epoch [3/5],  Loss: 0.7259\n",
            "Epoch [3/5],  Loss: 0.6620\n",
            "Epoch [3/5],  Loss: 0.7273\n",
            "Epoch [3/5],  Loss: 0.6597\n",
            "Epoch [3/5],  Loss: 0.7262\n",
            "Epoch [3/5],  Loss: 0.7294\n",
            "Epoch [3/5],  Loss: 0.7195\n",
            "Epoch [3/5],  Loss: 0.7012\n",
            "Epoch [3/5],  Loss: 0.6886\n",
            "Epoch [3/5],  Loss: 0.6924\n",
            "Epoch [3/5],  Loss: 0.6944\n",
            "Epoch [3/5],  Loss: 0.7072\n",
            "Epoch [3/5],  Loss: 0.6788\n",
            "Epoch [3/5],  Loss: 0.7179\n",
            "Epoch [3/5],  Loss: 0.6750\n",
            "Epoch [3/5],  Loss: 0.6758\n",
            "Epoch [3/5],  Loss: 0.7087\n",
            "Epoch [3/5],  Loss: 0.6769\n",
            "Epoch [3/5],  Loss: 0.6619\n",
            "Epoch [3/5],  Loss: 0.7239\n",
            "Epoch [3/5],  Loss: 0.7255\n",
            "Epoch [3/5],  Loss: 0.6689\n",
            "Epoch [3/5],  Loss: 0.7132\n",
            "Epoch [3/5],  Loss: 0.7164\n",
            "Epoch [3/5],  Loss: 0.7120\n",
            "Epoch [3/5],  Loss: 0.6925\n",
            "Epoch [3/5],  Loss: 0.6898\n",
            "Epoch [3/5],  Loss: 0.7019\n",
            "Epoch [3/5],  Loss: 0.7022\n",
            "Epoch [3/5],  Loss: 0.6874\n",
            "Epoch [4/5],  Loss: 0.6983\n",
            "Epoch [4/5],  Loss: 0.6818\n",
            "Epoch [4/5],  Loss: 0.7041\n",
            "Epoch [4/5],  Loss: 0.6819\n",
            "Epoch [4/5],  Loss: 0.6929\n",
            "Epoch [4/5],  Loss: 0.6834\n",
            "Epoch [4/5],  Loss: 0.6837\n",
            "Epoch [4/5],  Loss: 0.6663\n",
            "Epoch [4/5],  Loss: 0.6452\n",
            "Epoch [4/5],  Loss: 0.6650\n",
            "Epoch [4/5],  Loss: 0.6322\n",
            "Epoch [4/5],  Loss: 0.6295\n",
            "Epoch [4/5],  Loss: 0.8002\n",
            "Epoch [4/5],  Loss: 0.7611\n",
            "Epoch [4/5],  Loss: 0.7749\n",
            "Epoch [4/5],  Loss: 0.6379\n",
            "Epoch [4/5],  Loss: 0.7468\n",
            "Epoch [4/5],  Loss: 0.7475\n",
            "Epoch [4/5],  Loss: 0.6506\n",
            "Epoch [4/5],  Loss: 0.7233\n",
            "Epoch [4/5],  Loss: 0.6565\n",
            "Epoch [4/5],  Loss: 0.6538\n",
            "Epoch [4/5],  Loss: 0.7428\n",
            "Epoch [4/5],  Loss: 0.6581\n",
            "Epoch [4/5],  Loss: 0.7459\n",
            "Epoch [4/5],  Loss: 0.6613\n",
            "Epoch [4/5],  Loss: 0.7362\n",
            "Epoch [4/5],  Loss: 0.7353\n",
            "Epoch [4/5],  Loss: 0.7183\n",
            "Epoch [4/5],  Loss: 0.7149\n",
            "Epoch [4/5],  Loss: 0.6992\n",
            "Epoch [4/5],  Loss: 0.6853\n",
            "Epoch [4/5],  Loss: 0.6813\n",
            "Epoch [4/5],  Loss: 0.6803\n",
            "Epoch [4/5],  Loss: 0.7173\n",
            "Epoch [4/5],  Loss: 0.7033\n",
            "Epoch [4/5],  Loss: 0.6908\n",
            "Epoch [4/5],  Loss: 0.6892\n",
            "Epoch [4/5],  Loss: 0.6608\n",
            "Epoch [4/5],  Loss: 0.6984\n",
            "Epoch [4/5],  Loss: 0.6822\n",
            "Epoch [4/5],  Loss: 0.6935\n",
            "Epoch [4/5],  Loss: 0.7001\n",
            "Epoch [4/5],  Loss: 0.6852\n",
            "Epoch [4/5],  Loss: 0.7885\n",
            "Epoch [4/5],  Loss: 0.6818\n",
            "Epoch [4/5],  Loss: 0.6589\n",
            "Epoch [4/5],  Loss: 0.7059\n",
            "Epoch [4/5],  Loss: 0.7111\n",
            "Epoch [4/5],  Loss: 0.7117\n",
            "Epoch [4/5],  Loss: 0.6814\n",
            "Epoch [4/5],  Loss: 0.6833\n",
            "Epoch [4/5],  Loss: 0.6546\n",
            "Epoch [4/5],  Loss: 0.7001\n",
            "Epoch [4/5],  Loss: 0.7590\n",
            "Epoch [4/5],  Loss: 0.6641\n",
            "Epoch [4/5],  Loss: 0.6905\n",
            "Epoch [4/5],  Loss: 0.7160\n",
            "Epoch [4/5],  Loss: 0.7280\n",
            "Epoch [4/5],  Loss: 0.6843\n",
            "Epoch [4/5],  Loss: 0.6957\n",
            "Epoch [4/5],  Loss: 0.7123\n",
            "Epoch [4/5],  Loss: 0.6998\n",
            "Epoch [4/5],  Loss: 0.6827\n",
            "Epoch [4/5],  Loss: 0.6758\n",
            "Epoch [4/5],  Loss: 0.7156\n",
            "Epoch [4/5],  Loss: 0.7021\n",
            "Epoch [4/5],  Loss: 0.6803\n",
            "Epoch [4/5],  Loss: 0.6803\n",
            "Epoch [4/5],  Loss: 0.7017\n",
            "Epoch [4/5],  Loss: 0.7407\n",
            "Epoch [4/5],  Loss: 0.6708\n",
            "Epoch [4/5],  Loss: 0.7107\n",
            "Epoch [4/5],  Loss: 0.6879\n",
            "Epoch [4/5],  Loss: 0.6927\n",
            "Epoch [4/5],  Loss: 0.6730\n",
            "Epoch [4/5],  Loss: 0.6718\n",
            "Epoch [4/5],  Loss: 0.7251\n",
            "Epoch [4/5],  Loss: 0.7310\n",
            "Epoch [4/5],  Loss: 0.6610\n",
            "Epoch [4/5],  Loss: 0.7291\n",
            "Epoch [4/5],  Loss: 0.7234\n",
            "Epoch [4/5],  Loss: 0.6648\n",
            "Epoch [4/5],  Loss: 0.7201\n",
            "Epoch [4/5],  Loss: 0.7086\n",
            "Epoch [4/5],  Loss: 0.7093\n",
            "Epoch [4/5],  Loss: 0.7055\n",
            "Epoch [4/5],  Loss: 0.6829\n",
            "Epoch [4/5],  Loss: 0.7108\n",
            "Epoch [4/5],  Loss: 0.6958\n",
            "Epoch [4/5],  Loss: 0.6939\n",
            "Epoch [4/5],  Loss: 0.6904\n",
            "Epoch [4/5],  Loss: 0.7051\n",
            "Epoch [4/5],  Loss: 0.6847\n",
            "Epoch [4/5],  Loss: 0.7029\n",
            "Epoch [4/5],  Loss: 0.6741\n",
            "Epoch [4/5],  Loss: 0.7279\n",
            "Epoch [4/5],  Loss: 0.7074\n",
            "Epoch [4/5],  Loss: 0.6812\n",
            "Epoch [4/5],  Loss: 0.7104\n",
            "Epoch [5/5],  Loss: 0.6928\n",
            "Epoch [5/5],  Loss: 0.6786\n",
            "Epoch [5/5],  Loss: 0.6903\n",
            "Epoch [5/5],  Loss: 0.6772\n",
            "Epoch [5/5],  Loss: 0.7164\n",
            "Epoch [5/5],  Loss: 0.7191\n",
            "Epoch [5/5],  Loss: 0.7134\n",
            "Epoch [5/5],  Loss: 0.6828\n",
            "Epoch [5/5],  Loss: 0.6886\n",
            "Epoch [5/5],  Loss: 0.7073\n",
            "Epoch [5/5],  Loss: 0.6850\n",
            "Epoch [5/5],  Loss: 0.7148\n",
            "Epoch [5/5],  Loss: 0.6739\n",
            "Epoch [5/5],  Loss: 0.7196\n",
            "Epoch [5/5],  Loss: 0.7214\n",
            "Epoch [5/5],  Loss: 0.7075\n",
            "Epoch [5/5],  Loss: 0.6840\n",
            "Epoch [5/5],  Loss: 0.7068\n",
            "Epoch [5/5],  Loss: 0.7042\n",
            "Epoch [5/5],  Loss: 0.6917\n",
            "Epoch [5/5],  Loss: 0.6829\n",
            "Epoch [5/5],  Loss: 0.6827\n",
            "Epoch [5/5],  Loss: 0.7132\n",
            "Epoch [5/5],  Loss: 0.6583\n",
            "Epoch [5/5],  Loss: 0.7166\n",
            "Epoch [5/5],  Loss: 0.6648\n",
            "Epoch [5/5],  Loss: 0.7272\n",
            "Epoch [5/5],  Loss: 0.7213\n",
            "Epoch [5/5],  Loss: 0.6799\n",
            "Epoch [5/5],  Loss: 0.6681\n",
            "Epoch [5/5],  Loss: 0.7091\n",
            "Epoch [5/5],  Loss: 0.6726\n",
            "Epoch [5/5],  Loss: 0.6629\n",
            "Epoch [5/5],  Loss: 0.6555\n",
            "Epoch [5/5],  Loss: 0.7438\n",
            "Epoch [5/5],  Loss: 0.7440\n",
            "Epoch [5/5],  Loss: 0.7153\n",
            "Epoch [5/5],  Loss: 0.6681\n",
            "Epoch [5/5],  Loss: 0.6586\n",
            "Epoch [5/5],  Loss: 0.7041\n",
            "Epoch [5/5],  Loss: 0.6538\n",
            "Epoch [5/5],  Loss: 0.7411\n",
            "Epoch [5/5],  Loss: 0.7104\n",
            "Epoch [5/5],  Loss: 0.7162\n",
            "Epoch [5/5],  Loss: 0.6972\n",
            "Epoch [5/5],  Loss: 0.7070\n",
            "Epoch [5/5],  Loss: 0.6858\n",
            "Epoch [5/5],  Loss: 0.6831\n",
            "Epoch [5/5],  Loss: 0.6463\n",
            "Epoch [5/5],  Loss: 0.6073\n",
            "Epoch [5/5],  Loss: 0.6871\n",
            "Epoch [5/5],  Loss: 0.6732\n",
            "Epoch [5/5],  Loss: 0.9620\n",
            "Epoch [5/5],  Loss: 0.7189\n",
            "Epoch [5/5],  Loss: 0.7449\n",
            "Epoch [5/5],  Loss: 0.7279\n",
            "Epoch [5/5],  Loss: 0.6798\n",
            "Epoch [5/5],  Loss: 0.6735\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [5/5],  Loss: 0.7170\n",
            "Epoch [5/5],  Loss: 0.6721\n",
            "Epoch [5/5],  Loss: 0.7205\n",
            "Epoch [5/5],  Loss: 0.6685\n",
            "Epoch [5/5],  Loss: 0.6675\n",
            "Epoch [5/5],  Loss: 0.7171\n",
            "Epoch [5/5],  Loss: 0.7179\n",
            "Epoch [5/5],  Loss: 0.7007\n",
            "Epoch [5/5],  Loss: 0.6947\n",
            "Epoch [5/5],  Loss: 0.6729\n",
            "Epoch [5/5],  Loss: 0.6787\n",
            "Epoch [5/5],  Loss: 0.6794\n",
            "Epoch [5/5],  Loss: 0.7024\n",
            "Epoch [5/5],  Loss: 0.6611\n",
            "Epoch [5/5],  Loss: 0.6741\n",
            "Epoch [5/5],  Loss: 0.6032\n",
            "Epoch [5/5],  Loss: 0.5746\n",
            "Epoch [5/5],  Loss: 0.7390\n",
            "Epoch [5/5],  Loss: 0.6766\n",
            "Epoch [5/5],  Loss: 0.7046\n",
            "Epoch [5/5],  Loss: 0.6790\n",
            "Epoch [5/5],  Loss: 0.6862\n",
            "Epoch [5/5],  Loss: 0.6694\n",
            "Epoch [5/5],  Loss: 0.6791\n",
            "Epoch [5/5],  Loss: 0.7072\n",
            "Epoch [5/5],  Loss: 1.0267\n",
            "Epoch [5/5],  Loss: 0.7042\n",
            "Epoch [5/5],  Loss: 0.7476\n",
            "Epoch [5/5],  Loss: 0.6300\n",
            "Epoch [5/5],  Loss: 0.5619\n",
            "Epoch [5/5],  Loss: 0.7704\n",
            "Epoch [5/5],  Loss: 0.7804\n",
            "Epoch [5/5],  Loss: 0.6275\n",
            "Epoch [5/5],  Loss: 0.8004\n",
            "Epoch [5/5],  Loss: 0.6339\n",
            "Epoch [5/5],  Loss: 0.6561\n",
            "Epoch [5/5],  Loss: 0.5892\n",
            "Epoch [5/5],  Loss: 0.6724\n",
            "Epoch [5/5],  Loss: 0.7060\n",
            "Epoch [5/5],  Loss: 0.7068\n",
            "Epoch [5/5],  Loss: 0.4786\n",
            "Epoch [5/5],  Loss: 0.6969\n",
            "Variable containing:\n",
            "-0.8544 -0.5543\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8903 -0.5286\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8704 -0.5427\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8782 -0.5370\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.9076 -0.5167\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8646 -0.5468\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8672 -0.5449\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8585 -0.5513\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8609 -0.5495\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8609 -0.5495\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8752 -0.5392\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8565 -0.5528\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8758 -0.5387\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8513 -0.5566\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8700 -0.5429\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8768 -0.5380\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8672 -0.5449\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8799 -0.5358\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8520 -0.5561\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8695 -0.5433\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8608 -0.5496\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8682 -0.5443\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8655 -0.5462\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8724 -0.5412\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8615 -0.5491\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8685 -0.5440\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8640 -0.5473\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8640 -0.5473\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8513 -0.5566\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8838 -0.5331\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8682 -0.5442\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8666 -0.5454\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8728 -0.5409\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8703 -0.5427\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8721 -0.5414\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8580 -0.5517\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8497 -0.5578\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8627 -0.5482\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8832 -0.5335\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8509 -0.5569\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8510 -0.5569\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8601 -0.5501\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8625 -0.5484\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8699 -0.5430\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            "-0.8556 -0.5535\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "The accuracy percentage is 48.0\n",
            "\n",
            "\n",
            "The average loss is0.012734937197707527\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Pkotr1S3JfoP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}